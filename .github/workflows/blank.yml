- name: Scrape, build, download games, and push
  env:
    SYSTEM: ${{ github.event.inputs.system }}
    MAX_PAGES: ${{ github.event.inputs.max_pages }}
    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  run: |
    cat > scrape_and_build.py << 'PY'
    import os, json, re, time, requests, subprocess
    from bs4 import BeautifulSoup

    SYSTEM = os.environ.get("SYSTEM", "N64")
    MAX_PAGES = int(os.environ.get("MAX_PAGES", "50"))

    base_url = "https://vimm.net/vault/"
    params_template = {
        "mode": "adv",
        "p": "list",
        "system": SYSTEM,
        "sort": "Title",
        "sortOrder": "ASC",
    }

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Metadata-Indexer (GitHub Actions)",
        "Accept-Language": "en-US,en;q=0.9",
    })

    # Load previous index if exists
    index_path = os.path.join("vimm", "index.json")
    if os.path.exists(index_path):
        with open(index_path, "r", encoding="utf-8") as f:
            all_games = json.load(f)
        downloaded_ids = {g["id"] for g in all_games if g.get("file")}
    else:
        all_games = []
        downloaded_ids = set()

    for page in range(1, MAX_PAGES + 1):
        params = dict(params_template)
        params["page"] = page
        try:
            r = session.get(base_url, params=params, timeout=20)
        except Exception as e:
            print(f"Failed to fetch page {page}: {e}")
            break
        if r.status_code != 200:
            break

        soup = BeautifulSoup(r.text, "html.parser")
        links = soup.select('a[href*="/vault/?id="]')
        if not links:
            break

        for a in links:
            title = a.get_text(strip=True)
            href = a.get("href", "")
            url = f"https://vimm.net{href}" if href.startswith("/") else f"https://vimm.net/vault/{href}"
            m = re.search(r"[?&]id=(\d+)", url)
            game_id = m.group(1) if m else ""
            if not title or game_id in downloaded_ids:
                continue  # skip already downloaded

            safe_name = re.sub(r'[^A-Za-z0-9_. -]', '_', title)[:100]
            path = os.path.join("vimm", "games", SYSTEM, safe_name)
            os.makedirs(path, exist_ok=True)

            download_url = None
            game_file = None
            try:
                detail = session.get(url, timeout=20)
                if detail.status_code == 200:
                    detail_soup = BeautifulSoup(detail.text, "html.parser")
                    dl = detail_soup.select_one('a[href*="/download.php"]')
                    if dl and dl.get("href"):
                        dl_href = dl.get("href")
                        download_url = f"https://vimm.net{dl_href}" if dl_href.startswith("/") else dl_href

                if download_url:
                    with session.get(download_url, stream=True, timeout=60) as resp:
                        if resp.status_code == 200:
                            cd = resp.headers.get("Content-Disposition", "")
                            ext = ".bin"
                            m = re.search(r'filename="?([^";]+)"?', cd)
                            if m:
                                filename = m.group(1)
                                _, ext = os.path.splitext(filename)
                            else:
                                _, ext = os.path.splitext(download_url)

                            game_file = os.path.join(path, f"{safe_name}{ext}")
                            with open(game_file, "wb") as f:
                                for chunk in resp.iter_content(chunk_size=8192):
                                    if chunk:
                                        f.write(chunk)
                            print(f"Downloaded: {game_file}")
            except Exception as e:
                print(f"Failed to download {title}: {e}")

            game_info = {
                "system": SYSTEM,
                "title": title,
                "id": game_id,
                "url": url,
                "folder": path,
                "download_url": download_url,
                "file": game_file,
            }

            all_games.append(game_info)
            downloaded_ids.add(game_id)
            time.sleep(2)

    os.makedirs("vimm", exist_ok=True)
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(all_games, f, indent=2, ensure_ascii=False)
    print(f"Updated metadata + files for {len(all_games)} games to {index_path}")

    # Git commit & push new files
    try:
        subprocess.run(["git", "config", "--global", "user.name", "github-actions"], check=True)
        subprocess.run(["git", "config", "--global", "user.email", "github-actions@users.noreply.github.com"], check=True)
        subprocess.run(["git", "add", "vimm/"], check=True)
        subprocess.run(["git", "commit", "-m", f"Add/update {SYSTEM} games"], check=False)
        subprocess.run(["git", "push", "origin", "HEAD"], check=True)
        print("Pushed new games to repo")
    except Exception as e:
        print(f"Git push failed: {e}")
    PY

    python scrape_and_build.py
