name: Vimm Vault Scraper

on:
  workflow_dispatch:
    inputs:
      system:
        description: "Vault system (e.g., N64, GBA)"
        required: true
        default: "N64"
      max_pages:
        description: "Maximum pages to scrape"
        required: true
        default: 50

jobs:
  scrape-download:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 selenium webdriver-manager

      - name: Scrape, download, and commit games
        env:
          SYSTEM: ${{ github.event.inputs.system }}
          MAX_PAGES: ${{ github.event.inputs.max_pages }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}   # Use personal access token for push
        run: |
          cat > scrape_and_build.py << 'PY'
          import os
          import json
          import re
          import time
          import requests
          import subprocess
          from bs4 import BeautifulSoup
          from selenium import webdriver
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.common.by import By
          from webdriver_manager.chrome import ChromeDriverManager

          SYSTEM = os.environ.get("SYSTEM", "N64")
          MAX_PAGES = int(os.environ.get("MAX_PAGES", "50"))

          base_url = "https://vimm.net/vault/"
          params_template = {
              "mode": "adv",
              "p": "list",
              "system": SYSTEM,
              "sort": "Title",
              "sortOrder": "ASC",
          }

          session = requests.Session()
          session.headers.update({
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36",
              "Accept-Language": "en-US,en;q=0.9",
          })

          index_path = os.path.join("vimm", "index.json")
          if os.path.exists(index_path):
              with open(index_path, "r", encoding="utf-8") as f:
                  all_games = json.load(f)
              downloaded_ids = {g["id"] for g in all_games if g.get("file")}
          else:
              all_games = []
              downloaded_ids = set()

          chrome_options = Options()
          chrome_options.add_argument("--headless")
          chrome_options.add_argument("--no-sandbox")
          chrome_options.add_argument("--disable-dev-shm-usage")
          driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)

          for page in range(1, MAX_PAGES + 1):
              params = dict(params_template)
              params["page"] = page
              try:
                  r = session.get(base_url, params=params, timeout=20)
              except Exception as e:
                  print(f"Failed to fetch page {page}: {e}")
                  break
              if r.status_code != 200:
                  print(f"Stopped at page {page}, status code {r.status_code}")
                  break

              soup = BeautifulSoup(r.text, "html.parser")
              links = soup.select('a[href*="/vault/?id="]')
              if not links:
                  break

              for a in links:
                  title = a.get_text(strip=True)
                  href = a.get("href", "")
                  url = f"https://vimm.net{href}" if href.startswith("/") else f"https://vimm.net/vault/{href}"
                  m = re.search(r"[?&]id=(\d+)", url)
                  game_id = m.group(1) if m else ""
                  if not title or game_id in downloaded_ids:
                      continue

                  safe_name = re.sub(r'[^A-Za-z0-9_. -]', '_', title)[:100]
                  path = os.path.join("vimm", "games", SYSTEM, safe_name)
                  os.makedirs(path, exist_ok=True)

                  download_url = None
                  game_file = None

                  # Use Selenium to get JS-generated download link
                  try:
                      driver.get(url)
                      time.sleep(2)
                      dl_elem = driver.find_element(By.XPATH, "//a[contains(@href, 'download.php')]")
                      download_url = dl_elem.get_attribute("href")

                      if download_url:
                          with session.get(download_url, stream=True, timeout=60) as resp:
                              if resp.status_code == 200:
                                  cd = resp.headers.get("Content-Disposition", "")
                                  ext = ".bin"
                                  m = re.search(r'filename="?([^";]+)"?', cd)
                                  if m:
                                      filename = m.group(1)
                                      _, ext = os.path.splitext(filename)
                                  else:
                                      _, ext = os.path.splitext(download_url)

                                  game_file = os.path.join(path, f"{safe_name}{ext}")
                                  with open(game_file, "wb") as f:
                                      for chunk in resp.iter_content(chunk_size=8192):
                                          if chunk:
                                              f.write(chunk)
                                  print(f"Downloaded: {game_file}")
                  except Exception as e:
                      print(f"Failed to download {title}: {e}")

                  game_info = {
                      "system": SYSTEM,
                      "title": title,
                      "id": game_id,
                      "url": url,
                      "folder": path,
                      "download_url": download_url,
                      "file": game_file,
                  }

                  all_games.append(game_info)
                  downloaded_ids.add(game_id)
                  time.sleep(2)

          driver.quit()

          # Save metadata
          os.makedirs("vimm", exist_ok=True)
          with open(index_path, "w", encoding="utf-8") as f:
              json.dump(all_games, f, indent=2, ensure_ascii=False)
          print(f"Updated metadata + files for {len(all_games)} games to {index_path}")

          # Git commit & push
          os.environ["GIT_ASKPASS"] = "echo"
          os.environ["GIT_USERNAME"] = "github-actions"
          os.environ["GIT_PASSWORD"] = os.environ.get("GH_TOKEN")
          try:
              subprocess.run(["git", "add", "vimm/"], check=True)
              subprocess.run(["git", "commit", "-m", f"Add/update {SYSTEM} games"], check=False)
              subprocess.run(["git", "push", f"https://{os.environ['GH_TOKEN']}@github.com/${{ github.repository }}.git", "HEAD"], check=True)
              print("Pushed new games to repo")
          except Exception as e:
              print(f"Git push failed: {e}")
          PY

          python scrape_and_build.py
